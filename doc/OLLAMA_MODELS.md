# Ollama Model Compatibility Analysis

This document records empirical findings from running the klein-cli matrix test suite
(`testsuite/matrix_runner.sh`) against various Ollama models.

## Test Suite

Seven test cases cover the main capability areas:

| Test | What it measures |
|------|-----------------|
| `coding` | Single-turn file creation via `Write` tool |
| `fibonacci` | Multi-turn edit: create then modify a file via `Edit` tool |
| `long_text` | Long document processing and summarisation |
| `memory_state` | Multi-turn conversation with state retention |
| `refactoring` | Two-turn coordinated multi-step code refactoring |
| `research_scenario` | Text-only reasoning ‚Äî no tools required |
| `web_search` | Fetch and analyse a web page via `WebFetch` tool |

## Results

### ‚úÖ gpt-oss:20b ‚Äî Best overall performer

| coding | fibonacci | long_text | memory_state | refactoring | research_scenario | web_search |
|--------|-----------|-----------|--------------|-------------|-------------------|------------|
| ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |

**Score: 6/7**

- **Tool calling**: Native Ollama JSON tool calling ‚Äî works reliably
- **Thinking**: Supported (configurable via `"thinking": true/false` in backend JSON)
- **Context**: 128k tokens
- **Notes**: Best balance of capability and speed. Fails only `refactoring` ‚Äî a test that
  no current Ollama model passes (see known issue below).

---

### ‚úÖ gpt-oss:120b ‚Äî Most capable, very slow

| coding | fibonacci | long_text | memory_state | refactoring | research_scenario | web_search |
|--------|-----------|-----------|--------------|-------------|-------------------|------------|
| ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå |

**Score: 5/7**

- **Tool calling**: Native Ollama JSON tool calling ‚Äî reliable
- **Thinking**: Supported
- **Context**: 128k tokens
- **VRAM**: 60 GB (Q4_K_M) ‚Äî runs mostly on CPU with 12 GB GPU. Approximately 10 min/test.
- **Known issues**:
  - `refactoring`: Used `MultiEdit` with a malformed edits array; produced partial changes
  - `web_search`: Wikipedia stub page yields only category links; model cannot extract answer
    (same sparse-content issue as qwen3 larger models)
- **Notes**: Higher quality reasoning than 20b, but CPU-bound speed makes it impractical for
  interactive use on a 12 GB GPU machine.

---

### üü° qwen3 family ‚Äî Mostly capable

| Model | coding | fibonacci | long_text | memory_state | refactoring | research_scenario | web_search | Score |
|-------|--------|-----------|-----------|--------------|-------------|-------------------|------------|-------|
| qwen3:4b | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå | 4/7 |
| qwen3:8b | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | 4/7 |
| qwen3:14b | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå | 5/7 |
| qwen3:30b | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå | 4/7 |

- **Tool calling**: Native Ollama JSON tool calling ‚Äî works after the streaming fix (see below)
- **Thinking**: Supported; disabled by default via `"thinking": false` in backend JSON to avoid
  spending all output budget on `<think>` tokens
- **Context**: 40k tokens
- **Known issues**:
  - `fibonacci` (4b/8b): Edit loop corrupts file state ‚Äî model retries with stale `old_string`.
    The 8b produced a file with `Computational error: missing import for strconv` injected as
    a bare text line mid-code after repeated failed Edit attempts.
  - `coding` (30b): `think: false` API parameter is **not honoured** ‚Äî model outputs its
    reasoning as `<think>‚Ä¶</think>` text in the Content field, consuming all 2048 output
    tokens before reaching the Write tool call. Smaller models (4b/8b/14b) suppress thinking
    correctly. Workaround: prepend `/no_think` to the system prompt or increase `maxTokens`.
  - `refactoring`: All qwen3 models fail ‚Äî see universal refactoring issue below.
  - `web_search`: Fails on 4b, 14b, 30b (Wikipedia stub page too sparse). Passes on 8b
    where the fetched page happened to contain enough biography text.

#### Key fix: streaming tool calls

qwen3 sends `tool_calls` in the **first** streaming chunk (`done=false`), not in the final
`done=true` chunk. The original klein code only copied tool calls from the final chunk,
silently dropping all qwen3 tool calls and producing empty responses. Fixed in
`pkg/client/ollama/client.go` by accumulating tool calls across all streaming chunks.

---

### üü° MichelRosselli/GLM-4.5-Air ‚Äî Q3_K_M vs Q2_K comparison

#### GLM-4.5-Air:Q3_K_M ‚Äî **Recommended** quantization

| coding | fibonacci | long_text | memory_state | refactoring | research_scenario | web_search |
|--------|-----------|-----------|--------------|-------------|-------------------|------------|
| ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |

**Score: 6/7**

- **Tool calling**: Native Ollama JSON tool calling ‚Äî reliable
- **Thinking**: Supported (enabled via `"thinking": true` in backend JSON)
- **Context**: 128k tokens
- **Quantization**: Q3_K_M; community-uploaded GGUF of THUDM's GLM-4.5-Air
- **Known issues**:
  - `refactoring`: Universal Ollama failure ‚Äî see section below. Partially executes
    Turn 1 (updates `main()` to string IDs) but leaves the struct field as `int`.
    Turn 2 produces only 302 tokens and makes no tool calls ‚Äî thinking overhead
    likely consumes most of the 2048 token budget before reaching the Edit call.
- **Notes**: Best GLM-4.5-Air result. Matches gpt-oss:20b at 6/7. The extra parameters
  over Q2_K fix the fibonacci Turn-2 abandonment issue. Only fails `refactoring`, which
  no current Ollama model passes.

#### GLM-4.5-Air:Q2_K ‚Äî Lower VRAM, lower reliability

| coding | fibonacci | long_text | memory_state | refactoring | research_scenario | web_search |
|--------|-----------|-----------|--------------|-------------|-------------------|------------|
| ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |

**Score: 5/7**

- **Tool calling**: Native Ollama JSON tool calling ‚Äî works on single-turn tasks
- **Thinking**: Supported
- **Context**: 128k tokens
- **Quantization**: Q2_K (~4 GB VRAM); community-uploaded GGUF
- **Known issues**:
  - `fibonacci` (Turn 2): Model abandons the `Edit` tool and produces a text response
    instead. Unlike qwen3's stale-`old_string` loop, the model simply stops calling
    tools mid-task.
  - `refactoring`: Step 1 partially done (only `main()` updated); Step 2 skipped.
    Left a `%!d(string=1)` format verb artefact. Hallucinated a wrong absolute path
    before falling back to a relative path.
- **Notes**: Use Q3_K_M if VRAM allows. Q2_K degrades on multi-turn tool reliability.

---

### ‚ùå glm-4.7-flash ‚Äî Incompatible tool format

| coding | fibonacci | research_scenario | web_search |
|--------|-----------|-------------------|------------|
| ‚ùå | ‚ùå | ‚úÖ | ‚ùå |

- **Tool calling**: Uses XML tool call format (`<tool_call>‚Ä¶</tool_call>`) ‚Äî incompatible
  with Ollama's JSON tool calling API. Marked `Tool: false` in the model registry.
- **Thinking**: No
- **Context**: 128k tokens
- **Notes**: Can only do text-only tasks. Would need a custom XML parser to support tool
  use; not pursued.

---

### ‚ùå lfm2.5-thinking ‚Äî Too small

| coding | fibonacci | research_scenario | web_search |
|--------|-----------|-------------------|------------|
| ‚ùå | ‚ùå | ‚úÖ | ‚ùå |

- **Tool calling**: No ‚Äî 1.2B parameter model lacks capacity for structured tool call output
- **Thinking**: Label suggests thinking support, but effective output quality is very low
- **Context**: Unknown
- **Notes**: Model is too small to follow complex instructions or produce valid JSON tool
  calls. Only passes the simple text-only research task.

---

### ‚ùå rnj-1:8b ‚Äî No tool calling template

| coding | fibonacci | research_scenario | web_search |
|--------|-----------|-------------------|------------|
| ‚ùå | ‚ùå | ‚úÖ | ‚ùå |

- **Tool calling**: No ‚Äî based on Gemma3, whose GGUF template does not include a tool
  calling format
- **Thinking**: No
- **Context**: ~8k tokens
- **Notes**: DynamicCapabilityCheck correctly identifies it as non-tool-capable. No path
  to add tool support without a custom template.

---

### ‚õî qwen3.5 family ‚Äî Broken in Ollama (renderer bug)

- **Architecture**: `qwen35moe` ‚Äî MoE + Gated Delta Networks (newly released, different from qwen3)
- **System messages**: Crash the model runner with `500 Internal Server Error` ‚Äî the `qwen3.5`
  renderer/parser does not handle `{"role":"system"}` messages. The template is `{{ .Prompt }}`
  (raw prompt passthrough) rather than the standard chat template. Since klein always injects
  the skill as a system message, qwen3.5 is **completely non-functional** for our agent.
- **Tool calling**: Also crashes ‚Äî Ollama's pipeline uses the wrong tool-call format
  (Qwen3 Hermes JSON) when qwen3.5 was trained on Qwen3-Coder XML format. See
  [Ollama issue #14493](https://github.com/ollama/ollama/issues/14493).
- **Single-turn user messages**: Work correctly (no system role, no tools, no prior history).
- **Thinking**: Works when not combined with system messages.
- **Context**: 256K tokens
- **Recommendation**: Do not use until Ollama fixes the `qwen35moe` renderer. `model.go`
  marks the family as `Tool: false` to avoid crashes. Check Ollama release notes before
  re-enabling. A system-message-less run mode would be needed to test basic capabilities.

---

### ‚õî nemotron-3-nano ‚Äî Untestable (VRAM constraints)

- **Architecture**: Hybrid MoE (23 Mamba-2/MoE + 6 Attention layers); 31.6B total
  parameters with ~3.6B active per token
- **VRAM requirement**: Q4_K_M quantization is **24.27 GB** ‚Äî requires a GPU with ‚â•24 GB
  VRAM. The test machine (Alienware M15) has 12 GB VRAM; loading the model fails with
  `cudaMalloc failed: out of memory` even with no other models loaded.
- **Tool calling**: Unreliable in Ollama ‚Äî the model frequently wraps JSON tool calls in
  XML (`<tool_call>‚Ä¶</tool_call>`) roughly 50% of the time. Once the wrong format appears
  in the conversation history it tends to persist. Behaviour is similar to glm-4.7-flash.
- **Thinking**: Supported; configurable budget
- **Context**: 1M tokens
- **Recommendation**: If a GPU with ‚â•24 GB VRAM becomes available, test with
  `DynamicCapabilityCheck` first before adding to the backend matrix. The 1M context
  window is attractive for large-codebase tasks, but the XML/JSON tool call inconsistency
  is likely to cause failures similar to glm-4.7-flash. A smaller quantization (Q2_K or
  IQ2_M) may reduce VRAM requirements to ~12‚Äì14 GB at the cost of output quality.

---

## Summary

| Model | Tool calling | Thinking | Matrix score |
|-------|-------------|----------|--------------|
| gpt-oss:20b | ‚úÖ Native | ‚úÖ | 6/7 |
| gpt-oss:120b | ‚úÖ Native | ‚úÖ | 5/7 (slow ‚Äî CPU-bound) |
| qwen3:14b | ‚úÖ Native* | ‚úÖ | 5/7 |
| GLM-4.5-Air:Q3_K_M | ‚úÖ Native | ‚úÖ | 6/7 |
| GLM-4.5-Air:Q2_K | ‚úÖ Native | ‚úÖ | 5/7 |
| qwen3:30b | ‚úÖ Native* | ‚úÖ‚Ä† | 4/7 |
| qwen3:8b | ‚úÖ Native* | ‚úÖ | 4/7 |
| qwen3:4b | ‚úÖ Native* | ‚úÖ | 4/7 |
| glm-4.7-flash | ‚ùå XML only | ‚ùå | 1/4 |
| lfm2.5-thinking | ‚ùå | ‚ùå | 1/4 |
| rnj-1:8b | ‚ùå | ‚ùå | 1/4 |
| nemotron-3-nano | ‚ùì Unreliable | ‚úÖ | ‚õî OOM (24GB model, 12GB VRAM) |
| qwen3.5:35b | ‚ùå Crashes Ollama | ‚úÖ | ‚õî Ollama bug #14493 |

\* Required streaming fix: qwen3 sends tool calls in intermediate streaming chunks, not the final chunk.
‚Ä† qwen3:30b ignores `think: false` ‚Äî outputs reasoning as content, exhausting token budget before tool calls.
‚Ä° qwen3.5 tool calling crashes the model runner ‚Äî wrong format pipeline in Ollama. Fix pending.

### Known universal failure: `refactoring` test

All Ollama models fail the `refactoring` test. Root causes observed:
- Models call `todo_write` with incorrect field schema before reading the file
- Models produce text explanations of required changes instead of executing them via tools
- `MultiEdit` is called with an improperly structured edits array
- Even when some changes are applied, the check script's criteria (requiring all 6 specific
  changes) are not fully satisfied

This appears to be a combination of test difficulty (two-turn, multi-step, strict checklist)
and model limitations. The test may need relaxed pass criteria or a more guided prompt.

## Lessons Learned

1. **Streaming tool call placement matters**: Different models send `tool_calls` at different
   points in the streaming response. Always accumulate across all chunks.
2. **Model registry (`model.go`) is manual**: Capability flags (`Tool`, `Think`, `Vision`)
   must be verified empirically ‚Äî model cards are not always accurate.
3. **`think: false` is not universally honoured**: For qwen3:30b, the Ollama `think`
   parameter is ignored and reasoning appears in the Content field. Larger quantizations of
   the same family may have different template behaviour.
4. **Tool result role must be `"tool"`**: Sending tool results as `"user"` messages breaks
   native tool calling for strict models.
5. **Edit loops need escape hatches**: Multi-step edit tasks degrade when a failed `Edit`
   leaves the model unable to recover. `IterationAdvisor` now injects a re-read hint after
   the first failure on a file (via `FileSystemToolManager.GetToolState()`), down from the
   original threshold of 2+ consecutive failures.
6. **Wikipedia stub pages break web_search**: The target article for the web_search test
   contains only category links on smaller/denser models. A richer test URL or a fallback
   search step would improve reliability across all model sizes.
7. **Large models on insufficient VRAM are impractical**: gpt-oss:120b (60 GB) on a 12 GB
   GPU runs at ~10 min/test. Only useful if a GPU with sufficient VRAM is available.
8. **New Ollama architectures may have deeply broken chat support**: qwen3.5 uses a new
   `qwen35moe` architecture with a custom renderer/parser. Ollama's initial implementation
   crashes on both system messages and tool calls ‚Äî making the model completely unusable for
   our agent (which always injects the skill as a system message). Always verify basic
   multi-turn chat with a system message before adding a new model family to `model.go`.
